mkdir: cannot create directory ‘../trained_weights/quickdraw_resnet50_eval’: File exists
Using /home2/gftw82/.cache/torch_extensions as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home2/gftw82/.cache/torch_extensions/rasterize_cuda/build.ninja...
Building extension module rasterize_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module rasterize_cuda...
[*] Using device: cuda:0
[*] Created a new test dataset: ../quickdraw
[*] Number of categories: 345
[*] ResNet50Backbone: use pretrained conv1 with 3 input channels
[*] Model Parameters:
[*]  Trainable Module rnn
[*]    rnn.weight_ih_l0: torch.Size([2048, 3])
[*]    rnn.weight_hh_l0: torch.Size([2048, 512])
[*]    rnn.bias_ih_l0: torch.Size([2048])
[*]    rnn.bias_hh_l0: torch.Size([2048])
[*]    rnn.weight_ih_l0_reverse: torch.Size([2048, 3])
[*]    rnn.weight_hh_l0_reverse: torch.Size([2048, 512])
[*]    rnn.bias_ih_l0_reverse: torch.Size([2048])
[*]    rnn.bias_hh_l0_reverse: torch.Size([2048])
[*]    rnn.weight_ih_l1: torch.Size([2048, 1024])
[*]    rnn.weight_hh_l1: torch.Size([2048, 512])
[*]    rnn.bias_ih_l1: torch.Size([2048])
[*]    rnn.bias_hh_l1: torch.Size([2048])
[*]    rnn.weight_ih_l1_reverse: torch.Size([2048, 1024])
[*]    rnn.weight_hh_l1_reverse: torch.Size([2048, 512])
[*]    rnn.bias_ih_l1_reverse: torch.Size([2048])
[*]    rnn.bias_hh_l1_reverse: torch.Size([2048])
[*]    attend_fc.weight: torch.Size([3, 1024])
[*]    attend_fc.bias: torch.Size([3])
[*]  None-Trainable Module conv
[*]    conv1.weight: torch.Size([64, 3, 7, 7])
[*]    bn1.weight: torch.Size([64])
[*]    bn1.bias: torch.Size([64])
[*]    layer1.0.conv1.weight: torch.Size([64, 64, 1, 1])
[*]    layer1.0.bn1.weight: torch.Size([64])
[*]    layer1.0.bn1.bias: torch.Size([64])
[*]    layer1.0.conv2.weight: torch.Size([64, 64, 3, 3])
[*]    layer1.0.bn2.weight: torch.Size([64])
[*]    layer1.0.bn2.bias: torch.Size([64])
[*]    layer1.0.conv3.weight: torch.Size([256, 64, 1, 1])
[*]    layer1.0.bn3.weight: torch.Size([256])
[*]    layer1.0.bn3.bias: torch.Size([256])
[*]    layer1.0.downsample.0.weight: torch.Size([256, 64, 1, 1])
[*]    layer1.0.downsample.1.weight: torch.Size([256])
[*]    layer1.0.downsample.1.bias: torch.Size([256])
[*]    layer1.1.conv1.weight: torch.Size([64, 256, 1, 1])
[*]    layer1.1.bn1.weight: torch.Size([64])
[*]    layer1.1.bn1.bias: torch.Size([64])
[*]    layer1.1.conv2.weight: torch.Size([64, 64, 3, 3])
[*]    layer1.1.bn2.weight: torch.Size([64])
[*]    layer1.1.bn2.bias: torch.Size([64])
[*]    layer1.1.conv3.weight: torch.Size([256, 64, 1, 1])
[*]    layer1.1.bn3.weight: torch.Size([256])
[*]    layer1.1.bn3.bias: torch.Size([256])
[*]    layer1.2.conv1.weight: torch.Size([64, 256, 1, 1])
[*]    layer1.2.bn1.weight: torch.Size([64])
[*]    layer1.2.bn1.bias: torch.Size([64])
[*]    layer1.2.conv2.weight: torch.Size([64, 64, 3, 3])
[*]    layer1.2.bn2.weight: torch.Size([64])
[*]    layer1.2.bn2.bias: torch.Size([64])
[*]    layer1.2.conv3.weight: torch.Size([256, 64, 1, 1])
[*]    layer1.2.bn3.weight: torch.Size([256])
[*]    layer1.2.bn3.bias: torch.Size([256])
[*]    layer2.0.conv1.weight: torch.Size([128, 256, 1, 1])
[*]    layer2.0.bn1.weight: torch.Size([128])
[*]    layer2.0.bn1.bias: torch.Size([128])
[*]    layer2.0.conv2.weight: torch.Size([128, 128, 3, 3])
[*]    layer2.0.bn2.weight: torch.Size([128])
[*]    layer2.0.bn2.bias: torch.Size([128])
[*]    layer2.0.conv3.weight: torch.Size([512, 128, 1, 1])
[*]    layer2.0.bn3.weight: torch.Size([512])
[*]    layer2.0.bn3.bias: torch.Size([512])
[*]    layer2.0.downsample.0.weight: torch.Size([512, 256, 1, 1])
[*]    layer2.0.downsample.1.weight: torch.Size([512])
[*]    layer2.0.downsample.1.bias: torch.Size([512])
[*]    layer2.1.conv1.weight: torch.Size([128, 512, 1, 1])
[*]    layer2.1.bn1.weight: torch.Size([128])
[*]    layer2.1.bn1.bias: torch.Size([128])
[*]    layer2.1.conv2.weight: torch.Size([128, 128, 3, 3])
[*]    layer2.1.bn2.weight: torch.Size([128])
[*]    layer2.1.bn2.bias: torch.Size([128])
[*]    layer2.1.conv3.weight: torch.Size([512, 128, 1, 1])
[*]    layer2.1.bn3.weight: torch.Size([512])
[*]    layer2.1.bn3.bias: torch.Size([512])
[*]    layer2.2.conv1.weight: torch.Size([128, 512, 1, 1])
[*]    layer2.2.bn1.weight: torch.Size([128])
[*]    layer2.2.bn1.bias: torch.Size([128])
[*]    layer2.2.conv2.weight: torch.Size([128, 128, 3, 3])
[*]    layer2.2.bn2.weight: torch.Size([128])
[*]    layer2.2.bn2.bias: torch.Size([128])
[*]    layer2.2.conv3.weight: torch.Size([512, 128, 1, 1])
[*]    layer2.2.bn3.weight: torch.Size([512])
[*]    layer2.2.bn3.bias: torch.Size([512])
[*]    layer2.3.conv1.weight: torch.Size([128, 512, 1, 1])
[*]    layer2.3.bn1.weight: torch.Size([128])
[*]    layer2.3.bn1.bias: torch.Size([128])
[*]    layer2.3.conv2.weight: torch.Size([128, 128, 3, 3])
[*]    layer2.3.bn2.weight: torch.Size([128])
[*]    layer2.3.bn2.bias: torch.Size([128])
[*]    layer2.3.conv3.weight: torch.Size([512, 128, 1, 1])
[*]    layer2.3.bn3.weight: torch.Size([512])
[*]    layer2.3.bn3.bias: torch.Size([512])
[*]    layer3.0.conv1.weight: torch.Size([256, 512, 1, 1])
[*]    layer3.0.bn1.weight: torch.Size([256])
[*]    layer3.0.bn1.bias: torch.Size([256])
[*]    layer3.0.conv2.weight: torch.Size([256, 256, 3, 3])
[*]    layer3.0.bn2.weight: torch.Size([256])
[*]    layer3.0.bn2.bias: torch.Size([256])
[*]    layer3.0.conv3.weight: torch.Size([1024, 256, 1, 1])
[*]    layer3.0.bn3.weight: torch.Size([1024])
[*]    layer3.0.bn3.bias: torch.Size([1024])
[*]    layer3.0.downsample.0.weight: torch.Size([1024, 512, 1, 1])
[*]    layer3.0.downsample.1.weight: torch.Size([1024])
[*]    layer3.0.downsample.1.bias: torch.Size([1024])
[*]    layer3.1.conv1.weight: torch.Size([256, 1024, 1, 1])
[*]    layer3.1.bn1.weight: torch.Size([256])
[*]    layer3.1.bn1.bias: torch.Size([256])
[*]    layer3.1.conv2.weight: torch.Size([256, 256, 3, 3])
[*]    layer3.1.bn2.weight: torch.Size([256])
[*]    layer3.1.bn2.bias: torch.Size([256])
[*]    layer3.1.conv3.weight: torch.Size([1024, 256, 1, 1])
[*]    layer3.1.bn3.weight: torch.Size([1024])
[*]    layer3.1.bn3.bias: torch.Size([1024])
[*]    layer3.2.conv1.weight: torch.Size([256, 1024, 1, 1])
[*]    layer3.2.bn1.weight: torch.Size([256])
[*]    layer3.2.bn1.bias: torch.Size([256])
[*]    layer3.2.conv2.weight: torch.Size([256, 256, 3, 3])
[*]    layer3.2.bn2.weight: torch.Size([256])
[*]    layer3.2.bn2.bias: torch.Size([256])
[*]    layer3.2.conv3.weight: torch.Size([1024, 256, 1, 1])
[*]    layer3.2.bn3.weight: torch.Size([1024])
[*]    layer3.2.bn3.bias: torch.Size([1024])
[*]    layer3.3.conv1.weight: torch.Size([256, 1024, 1, 1])
[*]    layer3.3.bn1.weight: torch.Size([256])
[*]    layer3.3.bn1.bias: torch.Size([256])
[*]    layer3.3.conv2.weight: torch.Size([256, 256, 3, 3])
[*]    layer3.3.bn2.weight: torch.Size([256])
[*]    layer3.3.bn2.bias: torch.Size([256])
[*]    layer3.3.conv3.weight: torch.Size([1024, 256, 1, 1])
[*]    layer3.3.bn3.weight: torch.Size([1024])
[*]    layer3.3.bn3.bias: torch.Size([1024])
[*]    layer3.4.conv1.weight: torch.Size([256, 1024, 1, 1])
[*]    layer3.4.bn1.weight: torch.Size([256])
[*]    layer3.4.bn1.bias: torch.Size([256])
[*]    layer3.4.conv2.weight: torch.Size([256, 256, 3, 3])
[*]    layer3.4.bn2.weight: torch.Size([256])
[*]    layer3.4.bn2.bias: torch.Size([256])
[*]    layer3.4.conv3.weight: torch.Size([1024, 256, 1, 1])
[*]    layer3.4.bn3.weight: torch.Size([1024])
[*]    layer3.4.bn3.bias: torch.Size([1024])
[*]    layer3.5.conv1.weight: torch.Size([256, 1024, 1, 1])
[*]    layer3.5.bn1.weight: torch.Size([256])
[*]    layer3.5.bn1.bias: torch.Size([256])
[*]    layer3.5.conv2.weight: torch.Size([256, 256, 3, 3])
[*]    layer3.5.bn2.weight: torch.Size([256])
[*]    layer3.5.bn2.bias: torch.Size([256])
[*]    layer3.5.conv3.weight: torch.Size([1024, 256, 1, 1])
[*]    layer3.5.bn3.weight: torch.Size([1024])
[*]    layer3.5.bn3.bias: torch.Size([1024])
[*]    layer4.0.conv1.weight: torch.Size([512, 1024, 1, 1])
[*]    layer4.0.bn1.weight: torch.Size([512])
[*]    layer4.0.bn1.bias: torch.Size([512])
[*]    layer4.0.conv2.weight: torch.Size([512, 512, 3, 3])
[*]    layer4.0.bn2.weight: torch.Size([512])
[*]    layer4.0.bn2.bias: torch.Size([512])
[*]    layer4.0.conv3.weight: torch.Size([2048, 512, 1, 1])
[*]    layer4.0.bn3.weight: torch.Size([2048])
[*]    layer4.0.bn3.bias: torch.Size([2048])
[*]    layer4.0.downsample.0.weight: torch.Size([2048, 1024, 1, 1])
[*]    layer4.0.downsample.1.weight: torch.Size([2048])
[*]    layer4.0.downsample.1.bias: torch.Size([2048])
[*]    layer4.1.conv1.weight: torch.Size([512, 2048, 1, 1])
[*]    layer4.1.bn1.weight: torch.Size([512])
[*]    layer4.1.bn1.bias: torch.Size([512])
[*]    layer4.1.conv2.weight: torch.Size([512, 512, 3, 3])
[*]    layer4.1.bn2.weight: torch.Size([512])
[*]    layer4.1.bn2.bias: torch.Size([512])
[*]    layer4.1.conv3.weight: torch.Size([2048, 512, 1, 1])
[*]    layer4.1.bn3.weight: torch.Size([2048])
[*]    layer4.1.bn3.bias: torch.Size([2048])
[*]    layer4.2.conv1.weight: torch.Size([512, 2048, 1, 1])
[*]    layer4.2.bn1.weight: torch.Size([512])
[*]    layer4.2.bn1.bias: torch.Size([512])
[*]    layer4.2.conv2.weight: torch.Size([512, 512, 3, 3])
[*]    layer4.2.bn2.weight: torch.Size([512])
[*]    layer4.2.bn2.bias: torch.Size([512])
[*]    layer4.2.conv3.weight: torch.Size([2048, 512, 1, 1])
[*]    layer4.2.bn3.weight: torch.Size([2048])
[*]    layer4.2.bn3.bias: torch.Size([2048])
[*]  Trainable Module fc
[*]    weight: torch.Size([345, 2048])
[*]    bias: torch.Size([345])
[*] Model Size: 32.63529M
Traceback (most recent call last):
  File "quickdraw_r2cnn_get_images.py", line 19, in <module>
    images = app.partial_run()
  File "/home2/gftw82/Sketch-R2CNN/scripts/base_eval.py", line 255, in partial_run
    loaded_paths = net.load(checkpoint)
  File "/home2/gftw82/Sketch-R2CNN/models/basemodel.py", line 101, in load
    net.load_state_dict(torch.load(net_path), strict=strict)
  File "/home2/gftw82/sketchr2cnnenv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1052, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for SeqEncoder:
	size mismatch for attend_fc.weight: copying a param with shape torch.Size([8, 1024]) from checkpoint, the shape in current model is torch.Size([3, 1024]).
	size mismatch for attend_fc.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([3]).
